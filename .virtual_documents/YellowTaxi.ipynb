


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt



# load the dataset
df = pd.read_csv("yello_taxi_data (1).csv")


df.head()


df.info()


df.describe()


df.isnull().sum()


df.shape





# Feature engineering (e.g., extracting hour of the day from the timestamp)
df['pickup_hour'] = pd.to_datetime(df['tpep_pickup_datetime']).dt.hour
df


df



# Convert pickup and dropoff times to datetime
df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])
df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])


# Feature engineering: Calculate trip duration in minutes
df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60



# Select numerical features
numerical_features = df.select_dtypes(include=['number'])

# Display the numerical features
print(numerical_features)


# Drop non-numerical features
df = df.select_dtypes(include=['number'])


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Create a heatmap
plt.figure(figsize=(10, 8))  # Adjust figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()


correlation_matrix


correlation_matrix['trip_duration']


# Calculate correlations with 'trip_duration'
correlations = df.corr()['trip_duration'].abs()

# Set a threshold for high correlation (adjust as needed)
high_correlation_threshold = 0.5

# Select features with high correlation to 'trip_duration'
high_correlated_features = correlations[correlations > high_correlation_threshold].index.tolist()

# Remove 'trip_duration' itself from the list
high_correlated_features.remove('trip_duration')

# Display the high correlated features
print(high_correlated_features)




# Calculate correlations with 'trip_duration'
correlations = df.corr()['trip_duration']

# Get absolute correlations for ranking
abs_correlations = correlations.abs()

# Select top 5-10 correlated features (adjust number as needed)
num_features_to_select = 10
top_correlated_features = abs_correlations.nlargest(num_features_to_select).index.tolist()

# Separate positively and negatively correlated features
positively_correlated = [feature for feature in top_correlated_features if correlations[feature] > 0]
negatively_correlated = [feature for feature in top_correlated_features if correlations[feature] < 0]

# Display results
print("Top Positively Correlated Features:")
print(positively_correlated)

print("\nTop Negatively Correlated Features:")
print(negatively_correlated)


# Combine selected features (adjust as needed)
selected_features = positively_correlated[:8] + negatively_correlated[:5]

# Selecting features and target variable
features = selected_features
target = 'trip_duration'

X = df[features]
y = df[target]

print("Selected Features:", features)


import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# ... (previous code for feature selection) ...

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models and hyperparameter grids
models = {
    'Linear Regression': (LinearRegression(), {}),
    'Ridge Regression': (Ridge(), {'alpha': [0.1, 1, 10]}),
    'Lasso Regression': (Lasso(), {'alpha': [0.1, 1, 10]}),
    'Decision Tree': (DecisionTreeRegressor(), {'max_depth': [None, 5, 10]}),
    'Random Forest': (RandomForestRegressor(), {'n_estimators': [5, 10], 'max_depth': [None, 5, 10]})
}

# Iterate over models, perform hyperparameter tuning, and evaluate
for model_name, (model, param_grid) in models.items():
    print(f"Evaluating {model_name}...")
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Best Hyperparameters: {grid_search.best_params_}")
    print(f"Mean Squared Error: {mse}")
    print(f"R-squared: {r2}")
    print("-" * 30)



# # Feature engineering (e.g., extracting hour of the day from the timestamp)
# df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])
# df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])
# df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60
# df['pickup_hour'] = df['pickup_datetime'].dt.hour
# df['dropoff_hour'] = df['dropoff_datetime'].dt.hour

# # Selecting features and target variable for trip duration prediction
# features_duration = ['trip_distance', 'pickup_hour', 'passenger_count']
# target_duration = 'trip_duration'

# X_duration = df[features_duration]
# y_duration = df[target_duration]

# # Splitting the data for trip duration prediction
# X_train_dur, X_test_dur, y_train_dur, y_test_dur = train_test_split(X_duration, y_duration, test_size=0.2, random_state=42)

# # Choosing and training the model for trip duration prediction
# model_duration = RandomForestRegressor(n_estimators=100, random_state=42)
# model_duration.fit(X_train_dur, y_train_dur)

# # Making predictions for trip duration
# y_pred_dur = model_duration.predict(X_test_dur)

# # Evaluating the model for trip duration
# mae_dur = mean_absolute_error(y_test_dur, y_pred_dur)
# mse_dur = mean_squared_error(y_test_dur, y_pred_dur)
# r2_dur = r2_score(y_test_dur, y_pred_dur)

# print(f'Trip Duration Prediction - MAE: {mae_dur}, MSE: {mse_dur}, R²: {r2_dur}')




# print(f'Total Amount Prediction - MAE: {mae_amt}, MSE: {mse_amt}, R²: {r2_amt}')



# Calculate correlations with 'total amount'
correlations = df.corr()['total_amount']

# Get absolute correlations for ranking
abs_correlations = correlations.abs()

# Select top 8-10 correlated features (adjust number as needed)
num_features_to_select = 10
top_correlated_features = abs_correlations.nlargest(num_features_to_select).index.tolist()

# Separate positively and negatively correlated features
positively_correlated = [feature for feature in top_correlated_features if correlations[feature] > 0]
negatively_correlated = [feature for feature in top_correlated_features if correlations[feature] < 0]

# Display results
print("Top Positively Correlated Features:")
print(positively_correlated)

print("\nTop Negatively Correlated Features:")
print(negatively_correlated)


top_correlated_features


# Combine selected features (adjust as needed)
selected_features = positively_correlated[:8] + negatively_correlated[:5]

# Remove 'total_amount' from the list
top_correlated_features = selected_features#.remove('total_amount')

# Selecting features and target variable
features = top_correlated_features
target = 'total_amount'

X = df[features]
y = df[target]



# Splitting the data for total amount prediction
X_train_amt, X_test_amt, y_train_amt, y_test_amt = train_test_split(X, y, test_size=0.3, random_state=42)

# Choosing and training the model for total amount prediction
model_amount = RandomForestRegressor(n_estimators=100, random_state=42)
model_amount.fit(X_train_amt, y_train_amt)

# Making predictions for total amount
y_pred_amt = model_amount.predict(X_test_amt)

# Evaluating the model for total amount
mae_amt = mean_absolute_error(y_test_amt, y_pred_amt)
mse_amt = mean_squared_error(y_test_amt, y_pred_amt)
r2_amt = r2_score(y_test_amt, y_pred_amt)


print(f'MAE: {mae_amt}, MSE: {mse_amt}, R²: {r2_amt}')





# Assuming 'total_amount' is the target variable in the DataFrame df
y_amount = df['total_amount']

# Assuming all other columns are features, or select specific features
X_amount = df.drop(columns=['total_amount'])
# Alternatively, you can select specific features
# X_amount = df[['feature1', 'feature2', 'feature3']]

model_amount = LinearRegression()

# Cross-validation for total amount prediction
cv_scores_amt = cross_val_score(model_amount, X_amount, y_amount, cv=5, scoring='neg_mean_absolute_error')
print(f'Cross-validated MAE for Total Amount Prediction: {-np.mean(cv_scores_amt)}')

from sklearn.ensemble import RandomForestRegressor

# Instantiate the RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Fit the model on the training data for total amount prediction
rf_model.fit(X_amount, y_amount)

# Get feature importances
importances = rf_model.feature_importances_

# Create a list of feature names and their corresponding importance
feature_importance = list(zip(X_amount.columns, importances))

# Sort the features by importance
feature_importance_sorted = sorted(feature_importance, key=lambda x: x[1], reverse=True)

# Display the sorted features and their importance
for feature, importance in feature_importance_sorted:
    print(f'Feature: {feature}, Importance: {importance}')







# Feature importance for trip duration prediction
importances_dur = model_duration.feature_importances_
indices_dur = np.argsort(importances_dur)[::-1]
feature_names_dur = X_duration.columns

print("Feature importance for Trip Duration Prediction:")
for i in range(len(importances_dur)):
    print(f"{feature_names_dur[indices_dur[i]]}: {importances_dur[indices_dur[i]]}")

# Feature importance for total amount prediction
importances_amt = model_amount.feature_importances_
indices_amt = np.argsort(importances_amt)[::-1]
feature_names_amt = X_amount.columns

print("Feature importance for Total Amount Prediction:")
for i in range(len(importances_amt)):
    print(f"{feature_names_amt[indices_amt[i]]}: {importances_amt[indices_amt[i]]}")




